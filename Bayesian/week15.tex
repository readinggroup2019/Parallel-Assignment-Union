\documentclass[a4paper, 11pt]{article}
\usepackage{comment} % enables the use of multi-line comments (\ifx \fi) 
\usepackage{lipsum} %This package just generates Lorem Ipsum filler text. 
\usepackage{fullpage} % changes the margin
\usepackage[a4paper, total={7in, 10in}]{geometry}
\usepackage[fleqn]{amsmath}
\usepackage{amssymb,amsthm}  % assumes amsmath package installed
\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}
\usepackage{graphicx}
\usepackage{tikz}
\usetikzlibrary{arrows}
\usepackage{verbatim}
\usepackage[numbered]{mcode}
\usepackage{float}
\usepackage{tikz}
    \usetikzlibrary{shapes,arrows}
    \usetikzlibrary{arrows,calc,positioning}

    \tikzset{
        block/.style = {draw, rectangle,
            minimum height=1cm,
            minimum width=1.5cm},
        input/.style = {coordinate,node distance=1cm},
        output/.style = {coordinate,node distance=4cm},
        arrow/.style={draw, -latex,node distance=2cm},
        pinstyle/.style = {pin edge={latex-, black,node distance=2cm}},
        sum/.style = {draw, circle, node distance=1cm},
    }
\usepackage{xcolor}
\usepackage{mdframed}
\usepackage[shortlabels]{enumitem}
\usepackage{indentfirst}
\usepackage{hyperref}
    
\renewcommand{\thesubsection}{\thesection.\alph{subsection}}

\newenvironment{problem}[2][Problem]
    { \begin{mdframed}[backgroundcolor=gray!20] \textbf{#1 #2} \\}
    {  \end{mdframed}}

% Define solution environment
\newenvironment{solution}
    {\textit{Solution:}}
    {}

\renewcommand{\qed}{\quad\qedsymbol}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}
%Header-Make sure you update this information!!!!
\noindent
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\large\textbf{} \hfill \textbf{Homework - 15}   \\
Email: @ruc.edu.cn \hfill ID: 201900015 \\
\normalsize Course: Bayesian \hfill Term: Spring 2020\\
Instructor: Dr. Luo and Dr. He  \hfill Due Date: $13^{th}$ July, 2020 \\
\noindent\rule{7in}{2.8pt}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{problem}{1}
	 The bayesian Lasso problem (derive the integral formula for the double exponential prior, and the Gibbs sampling algorithm)
\end{problem}
\begin{solution}



\end{solution}

\noindent\rule{7in}{2.8pt}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{problem}{2}
	Prove the inequality $\| f^{t+1} - f\|_{TV} \le \| \pi^t -\pi\|_{TV}$ on the lecture note.
\end{problem}
\begin{solution}


	
\end{solution}

\noindent\rule{7in}{2.8pt}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{problem}{9.20}
A model consists in the partial observation of normal vectors $Z=(X, Y) \sim$ $\mathcal{N}_{2}(0, \Sigma)$ according to a mechanism of random censoring. The corresponding data are given in Table 9.2. (Independent observations of $Z=(X, Y) \sim \mathcal{N}_{2}(0, \Sigma)$ with missing data (denoted $-)$)
\[
\begin{tabular}{lcccccccc}
\hline 
x & 1.17 & -0.98 & 0.18 & 0.57 & 0.21 & - & - & - \\
y & 0.34 & -1.24 & -0.13 & - & - & -0.12 & -0.83 & 1.64 \\
\hline
\end{tabular} 
\]

(a) Show that inference can formally be based on the likelihood
\[
\prod_{i=1}^{3}\left\{|\Sigma|^{-1 / 2} e^{-z_{i}^{t} \Sigma^{-1} z_{i} / 2}\right\} \sigma_{1}^{-2} e^{-\left(x_{4}^{2}+x_{5}^{2}\right) / 2 \sigma_{1}^{2}} \sigma_{2}^{-3} e^{-\left(y_{6}^{2}+y_{7}^{2}+y_{8}^{2}\right) / 2 \sigma_{2}^{2}}
\]

(b) Show that the choice of the prior distribution $\pi(\Sigma) \propto|\Sigma|^{-1}$ leads to difficulties given that $\sigma_{1}$ and $\sigma_{2}$ are isolated in the likelihood.

(c) Show that the missing components can be simulated through the following algorithm.

~\\
Algorithm A.37 -Normal Completion-\\
1. Simulate
\[
\begin{array}{ll}
X_{i}^{*} \sim \mathcal{N}\left(\rho \frac{\sigma_{1}}{\sigma_{2}} y_{i}, \sigma_{1}^{2}\left(1-\rho^{2}\right)\right) & (i=6,7,8) \\
Y_{i}^{*} \sim \mathcal{N}\left(\rho \frac{\sigma_{2}}{\sigma_{1}} x_{i}, \sigma_{2}^{2}\left(1-\rho^{2}\right)\right) & (I=4,5)
\end{array}
\]
2. Generate
\[
\Sigma^{-1} \sim \mathcal{W}_{2}\left(8, X^{-1}\right)
\]
with $X=\sum_{i=1}^{8} z_{i}^{*} z_{i}^{* t},$ the dispersion matrix of the completed data.
~\\

to derive the posterior distribution of the quantity of interest, $\rho$

(d) Propose a Metropolis-Hastings alternative based on a slice sampler.

\end{problem}
\begin{solution}
	
	
\end{solution}

\noindent\rule{7in}{2.8pt}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{problem}{9.21 (a) (b)}
	 Roberts and Rosenthal (2003) derive the polar slice sampler from the decomposition $\pi(x) \propto f_{0}(x) f_{1}(x)$ of a target distribution $\pi(x)$
	 
	(a) Show that the Gibbs sampler whose two steps are
	(i) to simulate $U$ uniformly on $(0, f_{1}(x))$ and
	(ii) to simulate $X$ from $f_{0}(x)$ restricted to the set of $x$ 's such that $f_{1}(x)>u$ is a valid MCMC algorithm with target distribution $\pi$.
	
	(b) Show that, if $f_{0}$ is constant, this algorithm is simply the slice sampler of Algorithm [A.31] in Chapter 8. (It is also called the uniform slice sampler in Roberts and Rosenthal $2003 .$ )
\end{problem}
\begin{solution}
	
	
\end{solution}

\noindent\rule{7in}{2.8pt}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%





\end{document}