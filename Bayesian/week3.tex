\documentclass[a4paper, 11pt]{article}
\usepackage{comment} % enables the use of multi-line comments (\ifx \fi)
\usepackage{lipsum} %This package just generates Lorem Ipsum filler text.
\usepackage{fullpage} % changes the margin
\usepackage[a4paper, total={7in, 10in}]{geometry}
\usepackage[fleqn]{amsmath}
\usepackage{amssymb,amsthm}  % assumes amsmath package installed
\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}
\usepackage{graphicx}
\usepackage{tikz}
\usetikzlibrary{arrows}
\usepackage{verbatim}
\usepackage[numbered]{mcode}
\usepackage{float}
\usepackage{tikz}
    \usetikzlibrary{shapes,arrows}
    \usetikzlibrary{arrows,calc,positioning}

    \tikzset{
        block/.style = {draw, rectangle,
            minimum height=1cm,
            minimum width=1.5cm},
        input/.style = {coordinate,node distance=1cm},
        output/.style = {coordinate,node distance=4cm},
        arrow/.style={draw, -latex,node distance=2cm},
        pinstyle/.style = {pin edge={latex-, black,node distance=2cm}},
        sum/.style = {draw, circle, node distance=1cm},
    }
\usepackage{xcolor}
\usepackage{mdframed}
\usepackage[shortlabels]{enumitem}
\usepackage{indentfirst}
\usepackage{hyperref}

\renewcommand{\thesubsection}{\thesection.\alph{subsection}}

\newenvironment{problem}[2][Problem]
    { \begin{mdframed}[backgroundcolor=gray!20] \textbf{#1 #2} \\}
    {  \end{mdframed}}

% Define solution environment
\newenvironment{solution}
    {\textit{Solution:}}
    {}


\renewcommand{\qed}{\quad\qedsymbol}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}
%Header-Make sure you update this information!!!!
\noindent
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\large\textbf{Your Name} \hfill \textbf{Homework - 3}   \\
Email:               \hfill ID: 201900015x \\
\normalsize Course: Bayesian Statistics \hfill Term: Spring 2020\\
% Instructor: Xiangyu Luo, Shiyuan He \hfill Due Date: $23^{rd}$ February, 2020 \\
\noindent\rule{7in}{2.8pt}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Problem 1
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{problem}{5.1}
  Studying: The files school1.dat, school2.dat and school3.dat contain data on the amount of time students from three high schools spent on studying or homework during an exam period. Analyze data from each of these schools separately, using the normal model with a conjugate prior distribution, in which $\left\{\mu_{0}=5, \sigma_{0}^{2}=4, \kappa_{0}=1, \nu_{0}=2\right\}$, and compute or approximate the following:\\
  a) posterior means and $95\%$ confidence intervals for the mean $\theta$ and standard deviation $\sigma$ from each school;\\
  b)the posterior probability that $ \theta_{i}<\theta_{j}<\theta_{k}$ for all six permutations  $ \{i, j, k\}$ of $\{1,2,3\}$;\\
  c) the posterior probability that $ Y_{i}<Y_{j}<Y_{k}$ for all six permutations $ \{i, j, k\} $ of $\{1,2,3\}$,  where $ \tilde{Y}_{i}$  is a sample from the posterior predictive distribution of school  i . \\
  d) Compute the posterior probability that  $\theta_{1}$ is bigger than both $ \theta_{2}$ and  $\theta_{3}$, and the posterior probability that $ \tilde{Y}_{1}$ is bigger than both $ \tilde{Y}_{2}
  $ and $  \tilde{Y}_{3} $.
\end{problem}
\begin{solution}
...
\end{solution}

\noindent\rule{7in}{2.8pt}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Problem 2
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{problem}{5.5}
  Unit information prior: Obtain a unit information prior for the normal model as follows: \\
  a) Reparameterize the normal model as $ p(y | \theta, \psi)$, where$  \psi=1 / \sigma^{2} $.  Write  out the log likelihood $l(\theta, \psi | \boldsymbol{y})=\sum \log p\left(y_{i} | \theta, \psi\right)$ in terms of $ \theta$  and $ \psi $\\
  b) Find a probability density $ p_{U}(\theta, \psi)$ so that $ \log p_{U}(\theta, \psi)=l(\theta, \psi | \boldsymbol{y}) / n + c$,  where  c  is a constant that does not depend on $ \theta $ or $ \psi$ . Hint: Write $ \sum\left(y_{i}-\theta\right)^{2}$ as $ \sum\left(y_{i}-\bar{y}+\bar{y}-\theta\right)^{2}=\sum\left(y_{i}-\bar{y}\right)^{2}+n(\theta-\bar{y})^{2}, \text { and recall  that } \log p_{U}(\theta, \psi)=\log p_{U}(\theta | \psi)+\log p_{U}(\psi)$ \\
  c) Find a probability density $ p_{U}(\theta, \psi | \boldsymbol{y})$  that is proportional to $ p_{U}(\theta, \psi) \times  p\left(y_{1}, \ldots, y_{n} | \theta, \psi\right)$ . It may be convenient to write this joint density as $ p_{U}(\theta | \psi, \boldsymbol{y}) \times p_{U}(\psi | \boldsymbol{y})$  . Can this joint density be considered a posterior  density?
\end{problem}
\begin{solution}
...
\end{solution}

\noindent\rule{7in}{2.8pt}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Problem 3
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{problem}{6.2}
  Mixture model: The file glucose.dat contains the plasma glucose con- centration of 532 females from a study on diabetes (see Exercise 7.6).\\
  a) Make a histogram or kernel density estimate of the data. Describe how this empirical distribution deviates from the shape of a normal distribution.\\
  b) Consider the following mixture model for these data: For each study  participant there is an unobserved group membership variable $ X_{i}$   which is equal to  1  or  2 with probability $ p$ and $ 1-p$ . If $ X_{i}=1 $ then $ Y_{i} \sim \text { normal }\left(\theta_{1}, \sigma_{1}^{2}\right)$, and if$ X_{i}=2$  then $ Y_{i} \sim \text { normal }\left(\theta_{2}, \sigma_{2}^{2}\right) $. Let $  p \sim \text { beta }(a, b), \theta_{j} \sim \text { normal }\left(\mu_{0}, \tau_{0}^{2}\right) \text { and } 1 / \sigma_{j} \sim \operatorname{gamma}\left(\nu_{0} / 2, \nu_{0} \sigma_{0}^{2} / 2\right) $ for both  j=1 and  j=2 . Obtain the full conditional distributions of$ \left(X_{1}, \ldots, X_{n}\right), p, \theta_{1}, \theta_{2}, \sigma_{1}^{2} $ and $ \sigma_{2}^{2}$\\
  c) Setting  a=b=1, $\mu_{0}=120, \tau_{0}^{2}=200, \sigma_{0}^{2}=1000 $ and $ \nu_{0}=10 $  implement the Gibbs sampler for at least  10,000  iterations. Let $ \theta_{(1)}^{(s)}=\min \left\{\theta_{1}^{(s)}, \theta_{2}^{(s)}\right\}$  and $ \theta_{(2)}^{(s)}=\max \left\{\theta_{1}^{(s)}, \theta_{2}^{(s)}\right\} $.  Compute and plot the autocorrelation functions of $ \theta_{(1)}^{(s)} $ and $ \theta_{(2)}^{(s)}$,  as well as their effective  sample sizes. \\
  d) For each iteration  s of the Gibbs sampler, sample a value $ x \sim  \text { binary }\left(p^{(s)}\right)$,  then sample  $\tilde{Y}^{(s)} \sim \operatorname{normal}\left(\theta_{x}^{(s)}, \sigma_{x}^{2(s)}\right) $. Plot a histogram or kernel density estimate for the empirical distribution of $ \tilde{Y}^{(1)}, \ldots, \tilde{Y}^{(S)},$ and compare to the distribution in part a). Discuss  the adequacy of this two-component mixture model for the glucose data.
\end{problem}
\begin{solution}
...
\end{solution}

\noindent\rule{7in}{2.8pt}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Problem 4
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{problem}{6.3}
  Probit regression: A panel study followed 25 married couples over a period of five years. One item of interest is the relationship between divorce rates and the various characteristics of the couples. For example, the researchers would like to model the probability of divorce as a function of age differential, recorded as the man’s age minus the woman’s age. The data can be found in the file divorce.dat. We will model these data with probit regression, in which a binary variable Yi is described in terms of an explanatory variable xi via the following latent variable model:
  \begin{center}
      $\begin{aligned} Z_{i} &=\beta x_{i}+\epsilon_{i} \\ Y_{i} &=\delta_{(c, \infty)}\left(Z_{i}\right) \end{aligned}$
  \end{center}
   where $ \beta$  and c are unknown coefficients, $ \epsilon_{1}, \ldots, \epsilon_{n} \sim \text { i.i.d. normal }(0,1)$ \ and $ \delta_{(c, \infty)}(z)=1 $ if  z>c  and equals zero otherwise. \\

  a) Assuming $ \beta \sim \text { normal }\left(0, \tau_{\beta}^{2}\right) $ obtain the full conditional distribution $  p(\beta | \boldsymbol{y}, \boldsymbol{x}, \boldsymbol{z}, c) $\\
   b) Assuming $ c \sim \operatorname{normal}\left(0, \tau_{c}^{2}\right)$,  show that $ p(c | \boldsymbol{y}, \boldsymbol{x}, \boldsymbol{z}, \beta)$  is a constrained  normal density, i.e. proportional to a normal density but constrained to lie in an interval. Similarly, show that $p\left(z_{i} | \boldsymbol{y}, \boldsymbol{x}, \boldsymbol{z}_{-i}, \beta, c\right)$  is proportional to a normal density but constrained to be either above  c  or  below  c,  depending on $ y_{i}$\\
   c) Letting $ \tau_{\beta}^{2}=\tau_{c}^{2}=16 $, implement a Gibbs sampling scheme that approximates the joint posterior distribution of  Z,$ \beta,$  and  c  (a method for sampling from constrained normal distributions is outlined in Section 12.1.1). Run the Gibbs sampler long enough so that the effective  sample sizes of all unknown parameters are greater than 1,000 (including the $ {Z_i}^{'}s$  is). Compute the autocorrelation function of the parameters  and discuss the mixing of the Markov chain. \\
  d) Obtain a $95 \%$posterior confidence interval for$\beta $ , as well as $\operatorname{Pr}(\beta> 0 | y, x)$


\end{problem}
\begin{solution}
...

\end{solution}

\noindent\rule{7in}{2.8pt}

\end{document}
